{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VocaCOA4F7u"
   },
   "source": [
    "# Task 3: Training Loop Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HNXZr-aV6Xju"
   },
   "source": [
    "**Distributed Data Parallel (DDP)** PyTorchâ€™s built-in features, such as DataParallel (DP) and DistributedDataParallel (DDP) offer accelerated training capabilities. It transparently performs distributed data parallel training.\n",
    "As an example that uses a torch.nn.Linear as the local model, it is wrapped with DDP, and then runs one forward pass, one backward pass, and an optimizer step on the DDP model. After that, parameters on the local model will be updated, and all models on different processes should be exactly the same.\n",
    "Library that can be import is from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "source: https://pytorch.org/docs/master/notes/ddp.html\n",
    "\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "\n",
    "def example(rank, world_size):\n",
    "    # create default process group\n",
    "    dist.init_process_group(\"gloo\", rank=rank, world_size=world_size)\n",
    "    # create local model\n",
    "    model = nn.Linear(10, 10).to(rank)\n",
    "    # construct DDP model\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    # define loss function and optimizer\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = optim.SGD(ddp_model.parameters(), lr=0.001)\n",
    "\n",
    "    # forward pass\n",
    "    outputs = ddp_model(torch.randn(20, 10).to(rank))\n",
    "    labels = torch.randn(20, 10).to(rank)\n",
    "    # backward pass\n",
    "    loss_fn(outputs, labels).backward()\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "def main():\n",
    "\n",
    "    world_size = 2\n",
    "    mp.spawn(example,\n",
    "        args=(world_size,),\n",
    "        nprocs=world_size,\n",
    "        join=True)\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    # Environment variables which need to be\n",
    "    # set when using c10d's default \"env\"\n",
    "    # initialization mode.\n",
    "    os.environ[\"MASTER_ADDR\"] = \"localhost\"\n",
    "    os.environ[\"MASTER_PORT\"] = \"29500\"\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GQNq42eMB4iA"
   },
   "source": [
    "**Fully Sharded Data Parallel (FSDP)** - This can be used to accelerate training huge models on larger batch sizes. It is a way of training models with parallel processing, similar to traditional data-parallel methods. However, unlike the usual approach that keeps separate copies of a model's parameters, gradients, and optimizer states for each GPU, FSDP divides and shares these states among the parallel workers. Additionally, it provides the option to move the divided model parameters to CPUs if needed.\n",
    "\n",
    "Normally, FSDP wraps model layers in a nested manner. This means that only the layers within a specific FSDP instance have to bring all the parameters to a single device during forward or backward computations. Once the computation is done, the gathered parameters are released right away. This freed-up memory is then available for the next layer's computation. This process helps save peak GPU memory, allowing for the possibility of training with a larger model size or a larger batch size.\n",
    "\n",
    "**Using FSDP in pytorch** - There are two ways to wrap a model with PyTorch FSDP. Auto wrapping serves as a seamless replacement for DDP, while manual wrapping requires only minor adjustments to the model definition code, offering the flexibility to experiment with intricate sharding strategies.\n",
    "\n",
    "Source:https://pytorch.org/blog/introducing-pytorch-fully-sharded-data-parallel-api/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mio7-Bgm86-P"
   },
   "outputs": [],
   "source": [
    "#Auto wrapping\n",
    "\n",
    "  from torch.distributed.fsdp import (\n",
    "    FullyShardedDataParallel,\n",
    "    CPUOffload,\n",
    "  )\n",
    "  from torch.distributed.fsdp.wrap import (\n",
    "    default_auto_wrap_policy,\n",
    "  )\n",
    "  import torch.nn as nn\n",
    "\n",
    "  class model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(8, 4)\n",
    "        self.layer2 = nn.Linear(4, 16)\n",
    "        self.layer3 = nn.Linear(16, 4)\n",
    "\n",
    "  model = DistributedDataParallel(model())\n",
    "  fsdp_model = FullyShardedDataParallel(\n",
    "    model(),\n",
    "    fsdp_auto_wrap_policy=default_auto_wrap_policy,\n",
    "    cpu_offload=CPUOffload(offload_params=True),\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pG26nIDAct2S"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMQ0om2tGSuD1x7mUjM7Ehm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
